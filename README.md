# LDA-Goodreads-Reviews
Trabajo de Fin de Grado, uso de modelos LDA, y aplicaci贸n a rese帽as de Goodreads.

Este repositorio contiene el c贸digo y documentaci贸n desarrollados como parte del Trabajo de Fin de Grado (TFG), titulado **"Aplicaci贸n de modelos LDA a la identificaci贸n de t贸picos en Rese帽as"**.

El objetivo principal es aplicar t茅cnicas de modelado de t贸picos, concretamente el modelo LDA (*Latent Dirichlet Allocation*), para descubrir t贸picos latentes en rese帽as de libros extra铆das de la plataforma Goodreads. Se analiza la distribuci贸n de t贸picos seg煤n g茅neros y valoraciones, se eval煤a la convergencia del modelo y se exploran relaciones entre t贸picos mediante divergencia KL y visualizaciones.

---

##  Dataset

Las rese帽as utilizadas provienen de una fuentes p煤blica:

1. **Goodreads Book Reviews Dataset**  
   > Ahmad. (2023). *Goodreads Book Reviews*. Kaggle.  
   > [https://www.kaggle.com/datasets/pypiahmad/goodreads-book-reviews1](https://www.kaggle.com/datasets/pypiahmad/goodreads-book-reviews1)

   Contiene m谩s de 15 millones de rese帽as de usuarios. Para este trabajo se utilizaron las siguientes columnas:
   - `user_id`
   - `book_id`
   - `review_text`
   - `rating` (filtrando aquellas con puntuaci贸n 0)

Citas:  
   - Mengting Wan, Julian McAuley, "[Item Recommendation on Monotonic Behavior Chains](https://github.com/MengtingWan/mengtingwan.github.io/raw/master/paper/recsys18_mwan.pdf)", in RecSys'18. [[bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/recsys/WanM18)]
- Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley, "[Fine-Grained Spoiler Detection from Large-Scale Review Corpora](https://github.com/MengtingWan/mengtingwan.github.io/raw/master/paper/acl19_mwan.pdf)", in ACL'19. [[bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/acl/WanMNM19)]
 

---

##  Estructura del Proyecto y Orden de Ejecuci贸n

### 1. **Filtrado Inicial y Selecci贸n de Datos**

- `reduce_only_english_no_cero.py`:  
  Elimina rese帽as que no est谩n en ingl茅s o con puntuaci贸n 0.

- `genero_reviews.py`:  
  Detecta el g茅nero de cada rese帽a.

- `Only_{}_review.py`:  
  Filtra las rese帽as para trabajar con un 煤nico g茅nero espec铆fico (p.ej. solo "fantas铆a").

### 2. **Preprocesado de Texto**

- `Preprocesado.py`:  
  Limpieza del texto (URLs, stopwords, saltos de l铆nea...) usando `nltk`, `gensim` y `spacy`.

- `Filtrado.R`:  
  Elimina palabras extremadamente frecuentes o raras del vocabulario.

- `filtrado_puntuaciones_y_minimo_rese帽as.R`:  
  Filtra usuarios que no han usado todas las puntuaciones o que tienen muy pocas rese帽as.

### 3. **Preparaci贸n para el Modelo LDA**

- `Elementos_LDA.R`:  
  Estructura los documentos y genera los elementos necesarios para la realizaci贸n del modelo LDA.

### 4. **Modelado LDA**

- `LDA_Lotes.py`:  
  Aplicaci贸n del modelo LDA por lotes para grandes vol煤menes de datos.

- `Unir_Lotes_LDA.py`:  
  Reagrupaci贸n de los lotes en una 煤nica salida final.

- `LDA.py`:  
  Versi贸n alternativa del modelo LDA completo sobre el conjunto procesado.

### 5. **An谩lisis y Visualizaci贸n de Resultados**

- `Distribuci贸n_t贸picos_puntuaci贸n.py`:  
  Analiza c贸mo se distribuyen los t贸picos en funci贸n de la puntuaci贸n dada por los usuarios.

- `Convergencia.py`:  
  Mide la convergencia de la proporci贸n de los distintos t贸picos en las rese帽as seg煤n la puntuaci贸n asignada.

- `Divergencia_KL.py`:  
  Calcula la divergencia de Kullback-Leibler entre t贸picos para detectar t贸picos transversales, as铆 mismo, realiza un cambio de escala para una mayor visualizaci贸n.

- `Nube_Palabras.py`:  
  Generaci贸n de nubes de palabras representativas por t贸pico y sentimiento.

---

##  Estructura de Carpetas

Todos los scripts de procesamiento, modelado y an谩lisis se encuentran organizados en la carpeta [`scripts/`](./scripts).  
All铆 podr谩s encontrar los archivos `.py` y `.R` utilizados en cada una de las fases descritas anteriormente.


##  Requisitos

### Python (>= 3.8)

Instala las librer铆as necesarias ejecutando:

```bash
pip install pandas numpy matplotlib seaborn wordcloud scipy gensim nltk spacy
```
Tambi茅n es necesario descargar recursos adicionales para nltk y spaCy. Ejecuta el siguiente c贸digo en tu entorno Python:
import nltk
```bash
nltk.download('stopwords')

import spacy
spacy.cli.download("en_core_web_sm")
```

### R
Instala los siguientes paquetes ejecutando este c贸digo en una consola de R:
```bash
install.packages(c("tidyverse", "tm", "progress", "rstudioapi", "stopwords", "dplyr"))
```

----------------Contacto---------------------

Para cualquier pregunta, sugerencia o problema relacionado con el uso de este repositorio, por favor contacta con [adriobre9@gmail.com].
